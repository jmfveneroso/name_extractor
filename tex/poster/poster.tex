\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\setcopyright{none}

\usepackage{booktabs} % For formal tables
\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}

\DeclareMathOperator*{\argmax}{argmax}

\begin{document}

\title{Entity name extraction from faculty directories}

\author{João Mateus de Freitas Veneroso}
\affiliation{%
  \institution{Universidade Federal de Minas Gerais}
  \streetaddress{Av. Pres. Antônio Carlos, 6627 - Pampulha}
  \city{Belo Horizonte}
  \state{MG}
  \postcode{31270-901}
  \country{Brazil}}
\email{jmfveneroso@gmail.com}

\author{Berthier Ribeiro-Neto}
\affiliation{%
  \institution{Universidade Federal de Minas Gerais}
  \streetaddress{Av. Pres. Antônio Carlos, 6627 - Pampulha}
  \city{Belo Horizonte}
  \state{MG}
  \postcode{31270-901}
  \country{Brazil}}
\email{berthier@dcc.ufmg.br}

\begin{abstract}

Public bibliographic databases hold invaluable data about the academic 
environment. However, researcher affiliation information is frequently 
missing or outdated. We propose a statistical data extraction method to acquire 
affiliation information directly from university websites and solve the 
name extraction task in general.
Previous approaches to web data extraction either lack in flexibility,
because wrappers do not generalize well across website tasks, or 
they lack in precision, because domain agnostic methods neglect 
useful properties of this particular application domain.
Our statistical approach solves the name extraction task with 
a good tradeoff between generality and precision. 
We conducted experiments over a collection of 152 faculty 
web pages in multiple languages from universities in 49 countries
and obtained 94.40\% precision, 97.61\% recall and 0.9597 F-measure.


\end{abstract}

% \keywords{Information extraction, statistical classifier, web data extraction}

\maketitle

% ==========================================
% Beginning of text.                       |
% ==========================================

\section{Introduction}

Tipically, Information Extraction 
tasks consist of mapping unstructured or poorly
structured data to a semantically well defined structure. The input is most commonly
composed of a set of documents that describe a group of entities in a similar manner.
The Information Extraction task consists of identifying these entities and 
organizing them according to a template. 
The most successful approaches at solving 
the problem of information extraction are documented in several surveys 
\cite{Laender2002, Sarawagi2008, Ferrara2014, Schulz2016, Varlamov2016}.

We are interested in the task of name extraction, particularly extracting researcher names from 
university websites to complement data from public databases such as the DBLP repository 
(http://dblp.uni-trier.de/) 
\footnote{
  This is useful, for instance, if one needs to compare the research output of 
  deparments in Germany, or study international publication patterns.
}.
We propose a probabilistic method to handle the name extraction problem in general, 
without having to rely on metadata from PDF papers.  

The name extraction problem is not different from a Named Entity Recognition problem, because
if we are able to recognize entities we may also map them to a structured format. However,
state-of-the-art named entity recognition approaches such as Conditional Random Fields do not 
perform so well in information extraction scenarios such as the name extraction problem,
because the text in web pages is usually insufficient to provide proper contextual information 
about the semantic category of a word. For instance, tabular data such as the one found in most 
faculty directories provides minimal textual context.
Our method of label assignment resembles a Naive Bayesian classifier without the assumption of 
token independence. We also rely as little as possible on contextual information to avoid
the mistakes of other classifiers. 


\section{Implementation}

Before applying the classifier over the web page content, we must run the input web page
through a series of pre-processing steps to clean the data as much as possible. At this
stage we remove useless tags, and account for malformed HTML. Spaces, tabs, newlines and 
special characters are used as separators in a tokenization stage, producing 
a list of tokens. Each token is saved as an object that holds the token value and a pointer
to its position in the DOM tree. Through the DOM Element pointer we can figure out the parent 
elements, nesting depth, siblings and other information that can be used to improve estimates 
in the probabilistic method we adopt.


\subsection{Name Extraction}

Let $ t = (t_1, t_2, ..., t_n) $ be a sequence of token objects obtained on the pre-processing
stage, and $ y = (y_1, y_2, ..., y_n) $ be a sequence of labels attributed to these tokens
where $ y_i $ can be either a "Name Label" (N), meaning that token $ t_i $ is a person's name, or
a "Word Label" (W), meaning that token $ t_i $ is a common word (not a person's name). Then,
the problem of extracting names from a sequence of tokens is just a series of binary classification 
problems. Considering that each token $ t_i $ has a probability $ P(t_i = y_i) $ of having label 
$ y_i $, the problem of finding an optimal sequence of labels $ y^* $ for a sequence of tokens 
$ t $ can be written as:

\begin{equation}
\\
y^* = \argmax_y P(t_1 = y_1, t_2 = y_2, ..., t_n = y_n)\\
\\
\label{eq:1}
\end{equation}

We may employ the chain rule to explore the relationship between joint and conditional 
probabilities. For ease of exposure, consider that $ P(Y_i) \equiv P(t_i=y_i) $ yielding:

\begin{equation}
\\
P(Y_1, Y_2, ..., Y_n) = P(Y_1) P(Y_2|Y_1) ... P(Y_n|Y_{n-1}, Y_{n-2}, ...)
\\
\label{eq:2}
\end{equation}

The conditional probabilities $ P(Y_i|Y_{i-1}, ...) $ are hard to
estimate, because the joint distribution depends both on the previous labels and the 
previous tokens. To simplify our modeling, let us assume that the probability that token $ t_i $ has 
label $ y_i $ depends on the values of previous labels but is independent of the previous tokens. That is, 
we assume that, given a sequence of tokens $ \{"John", "Smith"\} $, the conditional probability 
$ P("Smith"|"John"=Name) $ 
is equivalent to $ P("Smith"|Any\ name) $. In other words, this means that the probability of 
Smith being a last name is the same regardless of a person's first name, as long as we can make sure 
that the previous token is a name. By taking this assumption, Equation \ref{eq:2} becomes:

\begin{equation}
\\
P(Y_i|Y_{i-1}, Y_{i-2}, ...) = P(t_i|y_i, y_{i-1}, ...)P(y_i|y_{i-1}, y_{i-2}, ...) \\
\\
\label{eq:3}
\end{equation}

In Equation \ref{eq:3}, the probability $ P(t_i|y_i, y_{i-1}, ...) $ depends on the current and
previous labels. We make a simplifying assumption that $ P(t_i|y_i, y_{i-1}, ...) $ can
be approximated by $ P(t_i|y_i) $. With this assumption, Equation \ref{eq:3} becomes: 

\begin{equation}
\\
P(Y_i|Y_{i-1}, Y_{i-2}, ...) = P(t_i|y_i)P(y_i|y_{i-1}, y_{i-2}, ...) \\
\\
\label{eq:6}
\end{equation}

Finally, by replacing Equation \ref{eq:6} into Equation \ref{eq:2} and grouping together
the second terms of Equation \ref{eq:6} to form the joint probability 
$ P(y_1, y_2, \ldots, y_n) $, we obtain:

\begin{equation}
\\
P(Y_1, \ldots, Y_n) = P(y_1, \ldots, y_n) P(t_1|y_1)P(t_2|y_2) \ldots P(t_n|y_n) 
\label{eq:7}
\end{equation}

Equation \ref{eq:7} can be split into two parts: the prior $ P(y_1, y_2, \ldots, y_n) $
and the conditional token probabilities $ P(t_1|y_1)P(t_2|y_2) \ldots P(t_n|y_n) $.

\subsubsection{Prior probabilities}

By assuming a window of size 
$ k \leq n $ we may approximate the priors by calculating the
joint probability $ P(y_1, y_2, \ldots, y_k) $. We observed empirically that when a name 
token is found, it is very likely that the next token will also be a name. However, 
arbitrary sequences of non-name tokens do not alter significantly the
probabilities for the next token's label.
We can use this property to slide the window of $ k $ tokens over the entire sequence of tokens 
and estimate probabilities without deviating too much from the real distribution. We start at
the beginning of the token list and calculate probabilities for each possible sequence of labels,
taking the most likely one. If the first token is not a name we slide the window right by one token 
and repeat the process. If the first token is a name, then we extract all the tokens that are part of
that name starting from the first one and slide the window right by the number of name tokens extracted. 

\subsubsection{Conditional token probabilities}

For our experiments, the conditional token probabilities 
were obtained by maximum likelihood estimation with Laplace smoothing to account for tokens that
didn't occur in the corpus. Conditional token probabilities can be made more precise by incorporating 
features in equation \ref{eq:7}. We do that by changing the token conditional 
probabilities to:

\begin{equation}
\\
P(t_i, f_1, f_2, \ldots, f_n|y_i) = P(t_i|y_i)P(f_1|y_i) \ldots P(f_n|y_i)
\\
\label{eq:8}
\end{equation}

where $ f_i $ are features that are assumed to be independent.
Textual features take textual clues from context words and the current token value. 
Structural features infer token probabilities based on the surrounding HTML structure.
The textual features found to be useful in our model were: token value, token incidence 
and token length.
The structural features found to be useful in our model were: first and second HTML parents,
CSS class name and nesting depth.

\subsubsection{Secondary estimates}

HTML structure varies wildly between different websites, 
so we cannot extract useful probabilities looking at the entire collection. 
For example, if all names appear inside a <tr> tag in a given document 
it does not mean that names tend to appear inside <tr> tags rather than any other HTML tag
in other documents. However for that particular document we may be able to identify 
other names and exclude non name tokens by knowing that tokens that occur inside <tr> tags
have a higher probability of being names.

Given that the basic algorithm (without structural features) was able to extract a
satisfactory number of names from a web page with sufficient precision on a first run,
we may estimate probabilities for a structural feature $ P(f_j|y_i) $ by looking at
the extracted names. In fact, since we already attributed a label to every token in the 
web page, we can access the originating DOM element through the token object and estimate
feature probabilities by maximum likelihood, as we did for the token conditional probabilities.

\subsection{Experiments}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{ |l|l|l|l| }
      \hline
      Features & Precision & Recall & F-measure \\
      \hline
      None                  & 91.71\% & 91.39\%           & 0.9155          \\ 
      Textual               & 91.83\% (0.46) & 93.90\% (0.15)  & 0.9285 (0.23) \\
      Full                  & 94.37\% (0.05) & \textbf{97.61\% (0.01)} & \textbf{0.9596 (0.01)} \\
      \hline
    \end{tabular}
  \end{center}
  \caption{Name extraction experiment}
  \label{tab:results}
\end{table}

The test collection was a set of 152 manually labeled faculty directory web pages from
49 different countries with 11782 researcher names. These names are the target of our 
extraction procedure. We calculated the precision, recall and f-measures for a base model
with no additional features besides conditional token probabilities and priors, a model 
with only textual features and a full model with textual and structural features. All
measures were tested for statistical significance with a one tailed paired T-test in
comparison with the base model, the t-values are presented inside parenthesis next to each measure. 
The measures were obtained by the averaged results of a 5 fold cross validation run. Extracted 
names were only considered to be correct when they resulted on an exact match with the test data.

\section{Conclusion}

Our name extraction method achieved 94.40\% precision, 97.61\% recall 
and 0.9597 F-measure over the test corpus.
The model is tuned for the particular problem of name extraction, 
but we believe this result can be generalized to solve other data extraction 
problems. The algorithm is general enough to handle other types of information
extraction tasks without the need for too much tinkering.
The secondary estimates strategy was very successful in further improving the
base model. It can also be remodeled to fit other statistical classifiers since
it does not rely on any particular implementation details of our strategy.


% ==========================================
% End of text.                             |
% ==========================================

% \nocite{*}
\bibliographystyle{unsrt}
\bibliography{bibfile}

\end{document}
