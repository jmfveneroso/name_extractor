\documentclass[
  msc,proposal,
  oneside,
  hidecover,
  hidededication,
  hideack,
  hideepigraph,
  hidelof,
  hidelot,
  hideabstract,
  hidecover,
  extraporttitlepagefalse
]{ppgccufmg}    

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{type1ec}
\usepackage{graphicx}
\usepackage[a4paper,
  portuguese,
  bookmarks=true,
  bookmarksnumbered=true,
  linktocpage,
  colorlinks,
  citecolor=black,
  urlcolor=black,
  linkcolor=black,
  filecolor=black,
  ]{hyperref}
\usepackage[square]{natbib}

\begin{document}

\ppgccufmg{
  title={Web data extraction in semi-structured documents},
  authorrev={de Freitas Veneroso, João Mateus},
  cutter={D1234p},
  cdu={519.6*82.10},
  university={Federal University of Minas Gerais},
  course={Computer Science},
  address={Belo Horizonte},
  date={2017-11},
  advisor={Berthier Ribeiro de Araújo Neto},
  approval={img/approvalsheet.eps}
}

% ==========================================
% Beginning of text.                       |
% ==========================================

\chapter{Introduction}

Web data extraction is the task of automatically extracting structured information
from unstructured or semi-structured web documents. It is a subset of the broader
field of Information Extraction, and thus it faces many of the same challenges. 

Structured information such as that found in a well organized relational database must 
conform to an underlying data model, namely an abstract model that formalizes
the entities and relationships in a given application domain. Unstructured information
however is not organized according to a logical model and has useful information often 
permeated by large chunks of irrelevant data.

Tipically, Information Extraction tasks consist of mapping unstructured or poorly
structured information to a semantically well defined structure. The input is usually
composed of a set of documents that describe a group of entities in a similar manner 
while the Information Extraction task deals with identifying those entities and 
organizing them according to a template. 

As an example, consider a collection of 
novels and the task of identifying the name of the main character in each novel. 
For this task, the model must first identify proper nouns and then understand the 
text sufficiently to allow inference on the relative importance of each noun. 

To achieve such a goal it is often useful to employ methods developed in the disciplines 
of Information Retrieval and Natural Language Processing. The former has achieved a 
great deal of success in the task of classifying documents according to statistical 
properties and the latter led to huge improvements in modelling human language.
Many times, the various methods employed in these disciplines lead to different 
approaches in the field of Information Extraction.

In the scope of this work, we are interested in the semi-structured data usually
found in HTML web documents. Web documents most often lie in between the 
structured-unstructured data paradigm, meaning that they take a rather
relaxed approach in regard to formal structure. Hierarchy, element disposition,
class names, and other features related to document structure and indirectly associated
with the data itself are valuable information in the task of identifying entities and 
determining relationships. So much that many times they the major source of information 
for classification purposes such as when extracting information from standardized tables.
However, far from a structured database, web documents usually provide very limited structure
to otherwise unstructured data such as that found in free text. 

Take for example the
staff page for the intelligent robotics laboratory of Osaka University shown in figure 1.
Say we want to extract the name, position, email and picture of all members. It is easy to see
there is some sort of structure to the information we want to extract from this particular 
website, however, parts of the information are missing, repeated or disposed differently for
each particular member. If we want to go further it may be necessary to only extract information
from full members, a task that may pose a harder challenge. We may use grouping similarity 
combined with textual information in order to properly identify the desired entities.
In many cases the HTML element relative position or CSS class name is sufficient to identify
occurrences of the same entity.
If we do a good enough job at this first task we may extrapolate our model to extract information
from similar web pages. The harder challenge lies in building a more general approach to collect
data with a similar underlying structure from many different types of web pages.

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{pics/jap_intelligent_robotics_lab}
  \caption{Example of semi structured information}
\end{figure}


Our current focused research interest regarding web data extraction is collecting
computer science researcher information from university websites. We need this 
information in order to compare the reputations of national and international 
research groups in the area of computer science with the academic reputation ranking
algorithm pscore. 

Pscore is a recent author level metric that attempts to measure academic reputation by calculating
Markov chain static distributions. It showed promising results in ranking publication venues and 
individual authors in the area of Computer Science using only information publically available in 
the DBLP database. However the DBLP database has sparse information about author affiliation and it 
is often outdated. To remedy this problem in the past few years researchers from UFMG have been 
laboriously collecting this data manually, but this process is tedious and error prone. Currently, 
only about 1\% of the DBLP authors have associated affiliation information, being most of them from 
USA and Brazil. Not nearly enough to allow broad international research group comparison.

Up to now, our goal has been to build an automatic information extraction system for
collecting author affiliation information from university websites. It has already 
achieved significant progress which shall be described further in the text. However, the ideas 
developed in this concrete case will be further improved in order to construct a more general
approach to the broader Information Extraction task.

The research here described hopes to contribute by proposing a novel approach to the main 
Information Extraction task, making the computer science affiliation database available for 
further research and testing the quality of pscore results regarding computer science research 
group comparison.

\chapter{Related Work}

Surveys
We provided a simple classification framework in which existing Web Data Extraction 
applications are grouped into two main classes, namely applications at the Enterprise 
level and at the Social Web level.

The design and implementation of Web Data Extraction systems has been discussed from different 
perspectives and it leverages on scientific methods coming from various disciplines including 
Information Retrieval, Machine Learning, and Natural Language Processing.

Some technological solutions which appear to be effective in some application contexts are not 
suitable in others.

Challenges:
Web Data Extraction techniques implemented in a Web Data Extraction system often require the help of human experts. A first challenge consists of providing a high degree of automation by reducing human efforts as much as possible. Human feedback, however, may play an important role in raising the level of accuracy achieved by a Web Data Extraction system.
A related challenge is, therefore, to identify a reasonable trade-off between the need of building highly automated Web Data Extraction procedures and the requirement of achieving accurate performance.

Web Data Extraction techniques should be able to process large volumes of data in relatively 
short time. This requirement is particularly stringent in the field of Business and 
Competitive Intelligence because a company needs to perform timely analysis of 
market conditions.

Applications in the field of Social Web or, more in general, those dealing with 
personal data must provide solid privacy guarantees. Therefore, potential (even 
if unintentional) attempts to violate user privacy should be timely and adequately 
identified and counteracted.

Approaches relying on Machine Learning often require a significantly large training 
set of manually labeled Web pages. In general, the task of labeling pages is 
time-expensive and error-prone and, therefore, in many cases we cannot assume 
the existence of labeled pages.

Oftentimes, a Web Data Extraction tool has to routinely extract data from a Web Data 
source which can evolve over time. Web sources are continuously evolving and structural 
changes happen with no forewarning, thus are unpredictable. Eventually, in real-world 
scenarios it emerges the need of maintaining these systems, that might stop working 
correctly if lacking of flexibility to detect and face structural modifications of 
related Web sources.

Debye

\chapter{Methodology}

Methodology here.

\chapter{Schedule}

Schedule here.

% ==========================================
% End of text.                             |
% ==========================================

\ppgccbibliography{bibfile}
\nocite{*}

\end{document}
